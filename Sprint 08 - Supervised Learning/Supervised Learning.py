#!/usr/bin/env python
# coding: utf-8

# 
# <div style="border-radius: 15px; border: 3px solid indigo; padding: 15px;">
# <b> Reviewer's comment 3</b>
# 
# 
# Thank you so much for your diligence! This is a hard topic, but everything is great in your project! You can find my new comments with digit 3. I do not have any questions, so the project can be accepted. Good luck! 
# 
#     
# </div>

# 
# <div style="border-radius: 15px; border: 3px solid indigo; padding: 15px;">
# <b> Reviewer's comment 2</b>
# 
# 
# Thank you for sending a new version of the project ðŸ˜Š I see a lot of updates! I've left a few comments titled as **Reviewer's comment 2**. Please take a look :) 
# 
#     
# 
#     
# </div>

# <div style="border-radius: 15px; border: 3px solid indigo; padding: 15px;">
# <b> Reviewer's comment</b>
#     
# Hi, I am a reviewer on this project. 
#     
# 
# Before we start, I want to pay your attention to the color marking:
#     
# <div class="alert alert-success" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <b> Reviewer's comment </b>
#     
# Great solutions and ideas that can and should be used in the future are in green comments.   
# </div>    
#     
#     
# <div class="alert alert-warning" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <b> Reviewer's comment </b>
# 
# Yellow color indicates what should be optimized. This is not necessary, but it will be great if you make changes to this project.
# </div>      
#     
#     
# <div class="alert alert-danger" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <b> Reviewer's comment </b>
# 
# Issues that need to be corrected to get right results are indicated in red comments. Note that the project cannot be accepted until these issues are resolved.
# </div>    
# 
# <hr>
#     
# **Please, use some color other than those listed to highlight answers to my comments.**
# I would also ask you **not to change, move or delete my comments** so that it would be easier for me to navigate during the next review.
#     
# In addition, my comments are defined as headings. 
# They can mess up the content; however, they are convenient, since you can immediately go to them. I will remove the headings from my comments in the next review. 
#    
#     
#     
# <hr>
#     
# <font color='dodgerblue'>**A few words about the project:**</font> you did a good job here, thank you so much! You compared several models and received scores, which is great. However, there are several issues we need to fix. I wrote everything in my comments. Please take a look.
#     
#     
#     
# 
# </div>

# <b> Project Preface </b>
# 
# <b> Project Title: Customer Churn Prediction for Beta Bank </b>
# 
# <b> Project Description: </b>
# 
# In the competitive banking sector, retaining existing customers is often more cost-effective than acquiring new ones. Beta Bank is facing a challenge with customer churnâ€”clients are leaving the bank gradually each month. To address this issue, we aim to build a predictive model that identifies customers likely to leave the bank in the near future.
# 
# The project involves developing a machine learning model to predict customer churn based on historical data of customer behavior and account activity. The dataset includes various features such as credit score, geographic location, account balance, and more. The goal is to analyze these features and build a model that accurately predicts whether a customer will exit the bank.
# 
# <b> Objectives: </b>
# 
# <b> 1. Data Preparation: </b>
#     
#     Download and Load Data: Retrieve the dataset and load it into a DataFrame.
#     Data Cleaning and Preprocessing: Handle missing values, encode categorical variables, and normalize numerical features.
# 
# <b> 2. Initial Model Building: </b>
#     
#     Class Balance Examination: Investigate the distribution of churn versus non-churn classes.
#     Baseline Model: Train an initial model without addressing class imbalance and evaluate its performance.
# 
# <b> 3. Model Improvement: </b>
#     
#     Address Class Imbalance: Implement at least two techniques to handle class imbalance, such as oversampling, 
#     undersampling, or algorithmic adjustments.
#     Model Tuning: Experiment with different algorithms and hyperparameters to improve model performance.
# 
# <b> 4. Final Evaluation: </b>
#     
#     Performance Metrics: Evaluate the final model using F1 score and AUC-ROC metrics to ensure it meets the performance 
#     criteria.
#     Model Validation: Ensure the model is robust and generalizes well to unseen data.
# 
# <b> Expected Outcomes: </b>
# 
# <b> 1. Data Insights: </b>
#     
#     A clear understanding of the data, including preprocessing steps taken to clean and prepare it for modeling.
# 
# <b> 2. Baseline Model Performance: </b>
#     
#     An initial evaluation of the model's performance without class imbalance adjustments, providing a benchmark for further 
#     improvements.
# 
# <b> 3. Enhanced Model: </b>
#     
#     A model that effectively handles class imbalance and demonstrates improved performance metrics.
# 
# <b> 4. Final Model Evaluation: </b>
#     
#     A final model that achieves an F1 score of at least 0.59 and provides insights into its ability to distinguish between churn and non-churn customers (measured by AUC-ROC).
# 
# <b> Documentation and Reporting: </b>
# 
# Comprehensive documentation of the entire process, including data preparation, model building, class imbalance handling, and performance evaluation. Clear and organized code with explanations of the approach and findings.
# 
# By the end of this project, we expect to deliver a robust predictive model that can help Beta Bank identify high-risk customers, enabling targeted retention strategies and potentially reducing overall churn rates.

# <div class="alert alert-success" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <h2>   Reviewer's comment </h2>
#     
# Good introduction! 
#     
# </div>

# In[1]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, f1_score, roc_auc_score
from sklearn.utils import resample


# In[2]:


# Load the data into a DataFrame
df = pd.read_csv('/datasets/Churn.csv')

# Display the first few rows of the dataset
display(df.head())

# Display summary statistics and info about the dataset
print(df.info())
display(df.describe())


# <b> Initial Observations: </b>
# 
# <b> Missing Values: </b>
# Tenure column has 9091 non-null entries out of 10000, indicating that 909 entries have missing values. This needs to be addressed during data preprocessing.
# 
# <b> Data Types: </b>
# Most columns are either numerical (int64, float64) or categorical (object). Categorical columns like Geography and Gender need to be encoded for model use.
# 
# <b> Summary Statistics: </b>
# 
# <b> CreditScore   : </b> Ranges from 350 to 850 with a mean around 650.
# 
# <b> Age           : </b> Ranges from 18 to 92 with a mean of around 39.
# 
# <b> Balance       : </b> Ranges from 0.00 to 250,898.09 with a mean of around 76,486.
# 
# <b> NumOfProducts : </b> Ranges from 1 to 4 with a mean of about 1.53.
# 
# <b> Exited        : </b> Imbalance observed; the mean value of Exited is 0.2037, indicating that around 20.37% of customers have exited the bank.

# Check for duplicates in CustomerId column

# In[3]:


duplicates = df.duplicated(subset='CustomerId').sum()
print(f"Number of duplicate CustomerId entries: {duplicates}")

# Remove duplicate CustomerId if any
df.drop_duplicates(subset='CustomerId', inplace=True)


# <div class="alert alert-success" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <h2>   Reviewer's comment </h2>
#     
# Good observations. 
#     
#     
# </div>
# <div class="alert alert-danger" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <b> Reviewer's comment</b>
#     
# 
#     
# Are there any duplicates in `CustomerId` column? 
# 
# </div>
# <div class="alert alert-success" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <h2>   Reviewer's comment 2 </h2>
#     
# Very good! 
#     
# </div>

# Handle Missing Values: For this step, we'll handle missing values by filling them with a new category.

# In[4]:


# Handle missing values by filling with 'missing' category for categorical variables
df['Geography'].fillna('missing', inplace=True)
df['Gender'].fillna('missing', inplace=True)

# Fill missing numerical values with the median of the respective columns
df['CreditScore'].fillna(df['CreditScore'].median(), inplace=True)
df['Age'].fillna(df['Age'].median(), inplace=True)
df['Tenure'].fillna(df['Tenure'].median(), inplace=True)
df['Balance'].fillna(df['Balance'].median(), inplace=True)
df['NumOfProducts'].fillna(df['NumOfProducts'].median(), inplace=True)
df['HasCrCard'].fillna(df['HasCrCard'].median(), inplace=True)
df['IsActiveMember'].fillna(df['IsActiveMember'].median(), inplace=True)
df['EstimatedSalary'].fillna(df['EstimatedSalary'].median(), inplace=True)


# <div class="alert alert-success" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <h2> Reviewer's comment</h2>
#     
# You can also fill in the gaps with a new value thus introducing a new "category". 
#     
# </div>
# <div class="alert alert-warning" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <h2> Reviewer's comment </h2>
#     
# It is better to place descriptive comments in separate Makrdown cells. Comments in the code should be concise and answer the question *What are we doing?*, not *Why?* or *For what?*. Usually only complex code is commented where it is difficult to figure out what's going on. So you don't need to leave comments in a really simple code :)
# </div>

# Remove columns that are not useful for modeling, such as RowNumber, CustomerId, and Surname.

# In[5]:


# Drop irrelevant columns
df.drop(columns=['RowNumber', 'CustomerId', 'Surname'], inplace=True)


# In[6]:


# Set the style for seaborn
sns.set(style="whitegrid")

# Create subplots to visualize the distributions and outliers
fig, axes = plt.subplots(3, 2, figsize=(15, 15))

# Plot histogram and box plot for Balance
sns.histplot(df['Balance'], kde=True, ax=axes[0, 0])
axes[0, 0].set_title('Balance Distribution')
axes[0, 0].set_xlabel('Balance')
axes[0, 0].set_ylabel('Frequency')

sns.boxplot(x=df['Balance'], ax=axes[0, 1])
axes[0, 1].set_title('Balance Box Plot')
axes[0, 1].set_xlabel('Balance')

# Plot histogram and box plot for Age
sns.histplot(df['Age'], kde=True, ax=axes[1, 0])
axes[1, 0].set_title('Age Distribution')
axes[1, 0].set_xlabel('Age')
axes[1, 0].set_ylabel('Frequency')

sns.boxplot(x=df['Age'], ax=axes[1, 1])
axes[1, 1].set_title('Age Box Plot')
axes[1, 1].set_xlabel('Age')

# Plot histogram and box plot for EstimatedSalary
sns.histplot(df['EstimatedSalary'], kde=True, ax=axes[2, 0])
axes[2, 0].set_title('EstimatedSalary Distribution')
axes[2, 0].set_xlabel('Estimated Salary')
axes[2, 0].set_ylabel('Frequency')

sns.boxplot(x=df['EstimatedSalary'], ax=axes[2, 1])
axes[2, 1].set_title('EstimatedSalary Box Plot')
axes[2, 1].set_xlabel('Estimated Salary')

# Adjust layout
plt.tight_layout()
plt.show()

# Draw conclusions
print("Conclusions:")
print("1. Balance Distribution: The balance distribution appears to be right-skewed, with a significant number of customers having a balance of zero. The box plot indicates the presence of outliers at the higher end of the balance range.")
print("2. Age Distribution: The age distribution is roughly normal, with a slight right skew. The box plot shows some outliers at the higher end, indicating a few older customers.")
print("3. EstimatedSalary Distribution: The estimated salary distribution is fairly uniform, suggesting a wide range of salaries among the customers. The box plot indicates the presence of outliers at both the lower and higher ends of the salary range.")


# <b> 1. Balance Distribution: </b>
#     
#    <b> Histogram Analysis: </b> The histogram reveals that a substantial portion of customers have a balance of zero, which may indicate either new customers who haven't deposited funds yet or customers who do not maintain a balance. There's a noticeable right skew, indicating that a smaller number of customers have significantly higher balances.
#     
#    <b> Box Plot Analysis: </b> The box plot confirms the presence of outliers in the upper range of balances. These outliers represent customers with unusually high balances compared to the rest of the dataset. This could point to a small segment of high-net-worth individuals within the customer base.
# 
# <b> Conclusion: </b> The bank may have a diverse customer base, ranging from those who maintain minimal balances to a few with substantial funds. Understanding the reasons behind the zero balances could provide insights into customer behavior and potential areas for engagement or improvement.
# 
# <b> 2. Age Distribution: </b>
#     
#    <b> Histogram Analysis: </b> The age distribution is roughly normal but with a slight right skew, indicating a larger proportion of younger customers, particularly in the 30-40 age range. There's a gradual decrease in frequency as age increases, with very few customers over 60.
#     
#    <b> Box Plot Analysis: </b> The box plot shows some outliers among older customers, indicating that while most customers fall within a certain age range, there are some significantly older individuals.
# 
# <b> Conclusion: </b> The customer base appears to be concentrated among middle-aged individuals, possibly reflecting a demographic that is actively saving or investing. The presence of older customers might indicate long-term clients or those with specific needs. Tailoring services to different age groups could enhance customer satisfaction and retention.
# 
# <b> 3. Estimated Salary Distribution: </b>
#     
#    <b> Histogram Analysis: </b> The estimated salary distribution is relatively uniform, suggesting that customers come from a broad range of income levels. There isn't a significant skew, which means no particular salary range is overly represented or underrepresented.
#    
#    <b> Box Plot Analysis: </b> The box plot shows that most salaries fall within a broad range, with outliers at both the low and high ends. This suggests that while there is a central cluster of customers around a middle-income level, the bank also serves some individuals with very low or very high salaries.
# 
# <b> Conclusion: </b> The uniform distribution of estimated salaries indicates that the bank serves a wide demographic, from low-income individuals to high-income earners. This diversity could be an opportunity for the bank to offer tiered services or products catering to different financial needs and capabilities.
# 
# <b> Overall Implications: </b>
# The analysis of these distributions reveals a diverse customer base in terms of financial behavior, age, and income levels. This diversity can be leveraged by the bank to tailor its products and services, ensuring they meet the specific needs of different customer segments. For instance:
# 
#    <b> For zero-balance customers: </b> The bank could investigate if these are new customers or inactive accounts and strategize on how to encourage account engagement.
#    
#    <b> For middle-aged customers: </b> As they form the bulk of the customer base, services like mortgage loans, investment products, or retirement planning could be particularly appealing.
#    
#    <b> For income-diverse customers: </b> The bank could offer specialized financial advice or products for both high-income earners looking to invest and low-income customers needing savings and budgeting assistance.
#    
# By understanding these aspects of their customer base, the bank can enhance customer engagement, satisfaction, and retention.

# <div class="alert alert-success" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <h2> Reviewer's comment </h2>
#     
# Yes, we can drop them, since they do not influence the behavior. 
#     
# </div>
# <div class="alert alert-danger" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <b> Reviewer's comment</b>
#     
# Before we move on to the machine learning models, let's display some charts to evaluate the distributions of different columns we have. For instance, do `Balance`, `Age`, `Salary` columns look normal? Do they have any outliers? Don't forget to add titles and axes labels so that a reader can easily understand what's going on here. Conclusions are also important :)
#     
# </div>
# <div class="alert alert-success" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <h2>   Reviewer's comment 2 </h2>
#     
# Excellent!     
# </div>
# <div class="alert alert-warning" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <h2> Reviewer's comment 2 </h2>
#     
# 
# Axes labels such as `Count` or `Frequency` may seem unclear to a reader. 
#     
# </div>

# <b> Split the Data </b>

# In[8]:


from sklearn.model_selection import train_test_split

# Split the data into features and target
X = df.drop(columns=['Exited'])
y = df['Exited']

# Split the dataset into training, validation, and test sets with 3:1:1 proportion
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

print(f"Training set: {X_train.shape}, {y_train.shape}")
print(f"Validation set: {X_val.shape}, {y_val.shape}")
print(f"Test set: {X_test.shape}, {y_test.shape}")


# <b> Encode Categorical Variables </b>

# In[9]:


from sklearn.preprocessing import LabelEncoder

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Make copies of the datasets
X_train = X_train.copy()
X_val = X_val.copy()
X_test = X_test.copy()

# Encode 'Geography' and 'Gender'
for column in ['Geography', 'Gender']:
    X_train.loc[:, column] = label_encoder.fit_transform(X_train[column])
    X_val.loc[:, column] = label_encoder.transform(X_val[column])
    X_test.loc[:, column] = label_encoder.transform(X_test[column])


# <b> Scaling Numerical Features </b>

# In[10]:


from sklearn.preprocessing import StandardScaler

# Initialize StandardScaler
scaler = StandardScaler()

# List of numerical columns to scale
numerical_columns = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']

# Scale numerical features
X_train[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])
X_val[numerical_columns] = scaler.transform(X_val[numerical_columns])
X_test[numerical_columns] = scaler.transform(X_test[numerical_columns])


# <div class="alert alert-success" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <h2> Reviewer's comment </h2>
#     
# For machine learning, `OneHotEncoder` is much better than `get_dummies`, so I recommend using it in the next projects :) 
#     
# </div>
# <div class="alert alert-warning" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <b> Reviewer's comment </b>
#     
# - It is highly recommended that we encode data after we split it. Sometimes test sets contain one or more categories that were not included in a training set. In the future projects, please try to split the data first and then apply encoding. 
#     
# 
# 
# - If the columns we want to convert are not explicitly specified, `get_dummies` will convert all columns with categorical strings, which may lead to unexpected results if some numeric columns also contain categorical data represented in numerical form (if there's a numerical category displayed as [1, 2, 3, 2, ... ]).
# </div>
# <div class="alert alert-success" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <h2>   Reviewer's comment 3 </h2>
#     
# Well done!     
# </div>

# In[11]:


# Check the distribution of the target variable (Exited) to understand the imbalance.
df['Exited'].value_counts(normalize=True)


# <b> Class Balance Observation </b>
# 
# The output of the class balance examination is as follows:
# 
#     Class 0 (Did not exit): 79.63%
#     Class 1 (Exited): 20.37%
# 
# <b> Explanation: </b>
# 
#     Class Imbalance: The dataset is imbalanced, with significantly more customers who did not exit (79.63%) compared to those who did exit (20.37%). This imbalance can impact the performance of machine learning models, particularly if the model is biased toward predicting the majority class.

# <div class="alert alert-success" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <h2> Reviewer's comment</h2>
#     
# Yes, there's a noticeable imbalance.   
#     
# </div>

# <b> Model Training and Evaluation </b>

# In[12]:


from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, roc_auc_score

# Initialize logistic regression model with class_weight='balanced'
initial_model = LogisticRegression(class_weight='balanced', random_state=42)

# Train the model
initial_model.fit(X_train, y_train)

# Predict on the validation set
y_val_pred = initial_model.predict(X_val)

# Calculate the F1 score and AUC-ROC
f1 = f1_score(y_val, y_val_pred)
roc_auc = roc_auc_score(y_val, initial_model.predict_proba(X_val)[:, 1])

print(f'F1 Score on Validation Set: {f1:.2f}')
print(f'AUC-ROC on Validation Set: {roc_auc:.2f}')


# <b> Handling Class Imbalance </b>
# 
#    <b> 1. Using Class Weights </b>

# <div class="alert alert-warning" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <h2>   Reviewer's comment 3 </h2>
#     
# You used `class_weight='balanced'` in the previous cell, so you can move the title above :)     
# </div>

# In[13]:


from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

# Define the model with class weights
class_weights = {0: 1, 1: (y_train == 0).sum() / (y_train == 1).sum()}

# RandomForest model
rf_model = RandomForestClassifier(class_weight=class_weights, random_state=42)

# Logistic Regression model
lr_model = LogisticRegression(class_weight=class_weights, random_state=42)

# Train the models
rf_model.fit(X_train, y_train)
lr_model.fit(X_train, y_train)

# Predict and evaluate on the validation set
from sklearn.metrics import classification_report

rf_val_preds = rf_model.predict(X_val)
lr_val_preds = lr_model.predict(X_val)

print("RandomForest Classification Report:")
print(classification_report(y_val, rf_val_preds))

print("Logistic Regression Classification Report:")
print(classification_report(y_val, lr_val_preds))


# <b> Conclusion: </b> 
# 
# <b> RandomForest: </b> 
# 
# Better overall accuracy (0.86) and weighted F1 score (0.84) than Logistic Regression.
# Precision for class 1 is better (0.76), but recall is lower (0.45).
# The F1 score for class 1 (0.56) is closer to the target threshold but still below 0.59.
# Logistic Regression:
# 
# Better recall for class 1 (0.72), but precision is much lower (0.38).
# The F1 score for class 1 (0.50) is lower than RandomForest.
# 
# <b> Recommendations: </b> 
# Hyperparameter Tuning: Optimize hyperparameters for RandomForest to improve the recall and F1 score for class 1.
# Feature Engineering: Enhance features to improve model performance.
# Combine Models: Use ensemble methods to leverage the strengths of both models.
# Alternative Algorithms: Consider other algorithms like Gradient Boosting, which might offer better performance.

#    <b> 2. Adjusting Decision Threshold </b>

# In[14]:


from sklearn.metrics import f1_score

# Predict probabilities
rf_val_probs = rf_model.predict_proba(X_val)[:, 1]
lr_val_probs = lr_model.predict_proba(X_val)[:, 1]

# Adjust threshold
threshold = 0.4  # You can adjust this value

rf_val_preds_adj = (rf_val_probs > threshold).astype(int)
lr_val_preds_adj = (lr_val_probs > threshold).astype(int)

print("Adjusted RandomForest Classification Report:")
print(classification_report(y_val, rf_val_preds_adj))

print("Adjusted Logistic Regression Classification Report:")
print(classification_report(y_val, lr_val_preds_adj))


# <b> Conclusion: </b> 
# 
# <b> RandomForest: </b> 
# 
# The adjusted RandomForest model shows a good balance between precision and recall for class 1, with an F1 score of 0.61, meeting the target threshold.
# The overall accuracy and weighted F1 score are high, indicating a robust model.
# Logistic Regression:
# 
# The adjusted Logistic Regression model has high recall for class 1 but suffers from very low precision, resulting in a low F1 score of 0.47 for class 1.
# The overall accuracy and weighted F1 score are low, suggesting that the model is not well-balanced.
# Recommendations:
# 
# <b> Proceed with RandomForest: </b> 
# 
# The adjusted RandomForest model has achieved the target F1 score for class 1.
# You can finalize this model or further fine-tune it for even better performance.
# Further Optimize Logistic Regression:
# 
# Consider alternative techniques such as L1/L2 regularization adjustments, different solvers, or even a combination with other models in an ensemble if you still wish to use Logistic Regression.
# Model Ensemble:
# 
# Combine both models in an ensemble approach to potentially improve the overall performance and leverage the strengths of each model.

# <div class="alert alert-danger" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <b> Reviewer's comment</b>
#     
# 
# Since we are supposed to check the F1 score on test subset in the final test, we need 3 datasets here (training set, validation, and test) . Try to have 3:1:1 proportion. 
#     
#     
# </div>
# <div class="alert alert-danger" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <b> Reviewer's comment 2</b>
#     
# 
# Let's not read the data again.    
#     
# </div>

# <div class="alert alert-danger" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <b> Reviewer's comment</b>
#     
# 
# Here and further, please do not forget to update the code. We will need to use validation subset to estimate the F1 score. 
#     
#     
#     
# </div>
# <div class="alert alert-success" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <h2> Reviewer's comment</h2>
#     
# By the way, with the presence of imbalanced classes where one class significantly predominates, the Macro Average F1 Score may not reflect the true effectiveness of the model, as it gives equal weight to each class regardless of their size. Although the Weighted Average F1 Score accounts for class imbalance, it can inflate the F1 Score if the predominant class has a high F1 Score. This can mask poor performance for the smaller but important class. 
#     
#     
# It is better to use the regular F1 Score because it focuses on the importance of balanced evaluation of both precision and recall for each class. The regular F1 also provides a more realistic assessment of model performance in situations where misclassifying one class can have significant consequences. Please take a look at this score for the model you trained above:
# </div>
# <div class="alert alert-danger" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <b> Reviewer's comment 2</b>
#     
# 
# We should not use test subset here. We only evaluate models on validation subset.     
#     
#     
# </div>

# <b> Hyperparameter Tuning </b>

# In[15]:


from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import f1_score, classification_report

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'class_weight': ['balanced']
}

# Initialize RandomForestClassifier
rf_model = RandomForestClassifier(random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='f1')

# Fit GridSearchCV to the training data
grid_search.fit(X_train, y_train)

# Get the best parameters
best_params = grid_search.best_params_
print(f"Best parameters for RandomForest: {best_params}")

# Train the model with the best parameters
best_rf_model = grid_search.best_estimator_
best_rf_model.fit(X_train, y_train)

# Predict on the validation set
y_val_pred = best_rf_model.predict(X_val)

# Evaluate the model
print("Tuned RandomForest Classification Report:")
print(classification_report(y_val, y_val_pred))

# Calculate the F1 score
f1 = f1_score(y_val, y_val_pred)
print(f'F1 Score on Validation Set: {f1:.2f}')


# <div class="alert alert-success" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <h2>   Reviewer's comment 3 </h2>
#     
# Well done. Here you tune hyperparameters on imbalanced data, which is very important because, according to the task, we need to train a model on imbalanced data as well.
# 
# </div>
# 

# <b> Conclusion: </b> 
# Model Performance: The adjusted RandomForest model has achieved an F1 score of 0.62 for class 1, which is above the target threshold of 0.59. The model demonstrates good performance in both precision and recall for class 1, with a balanced overall performance.
# 
# <b> Further Steps: </b> 
# Validate Performance: You might want to evaluate the model on the test set to confirm that it generalizes well.
# Feature Engineering: Explore additional feature engineering or data preprocessing to potentially improve model performance further.
# 
# Ensemble Methods: Consider combining this model with others or using ensemble techniques to potentially boost performance.

# <div class="alert alert-success" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <h2> Reviewer's comment</h2>
#     
# Good. 
#     
#     
# </div>
# <div class="alert alert-danger" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <b> Reviewer's comment 2</b>
#     
# Same issue.     
#     
# </div>

# <b> Final Testing </b>

# In[17]:


from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import f1_score, roc_auc_score

# Define the parameter grid for Gradient Boosting
param_grid_gbm = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7]
}

# Initialize GridSearchCV for Gradient Boosting
gbm_grid_search = GridSearchCV(
    estimator=GradientBoostingClassifier(),
    param_grid=param_grid_gbm,
    cv=5,
    scoring='f1',
    n_jobs=-1
)

# Fit GridSearchCV on training data
gbm_grid_search.fit(X_train, y_train)

# Get the best Gradient Boosting model
best_gbm_model = gbm_grid_search.best_estimator_

# Combine training and validation sets
X_train_combined = pd.concat([X_train, X_val])
y_train_combined = pd.concat([y_train, y_val])

# Train the final model on the combined training and validation data
best_gbm_model.fit(X_train_combined, y_train_combined)

# Predict on the test set
y_test_pred = best_gbm_model.predict(X_test)

# Evaluate the model
f1_test = f1_score(y_test, y_test_pred)
roc_auc_test = roc_auc_score(y_test, best_gbm_model.predict_proba(X_test)[:, 1])

print(f'Best GradientBoosting F1 Score on Test Set: {f1_test:.2f}')
print(f'Best GradientBoosting AUC-ROC on Test Set: {roc_auc_test:.2f}')


# <b> Analysis of Final GradientBoosting Model Results: </b> 
#     
#     Performance Metrics:
#     F1 Score on Test Set: 0.59
# 
# This meets your target F1 score of 0.59 for class 1. It indicates a balance between precision and recall for the positive class.
# AUC-ROC on Test Set: 0.86
# 
# This indicates good overall performance in distinguishing between the classes. The AUC-ROC score of 0.86 suggests that the model is very good at ranking instances by their likelihood of belonging to the positive class.

# <div class="alert alert-success" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <h2>   Reviewer's comment 3 </h2>
#     
# Great! 
# </div>
# 

# <div class="alert alert-danger" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <b> Reviewer's comment 2</b>
#     
# Here and further, please save test subset for the final testing. We will conduct the final testing in the very end, after we choose the best model based on their F1 scores calculated on validation subset.  
#     
# </div>

# <b> Training and Hyperparameter Tuning: </b> Only done on the training and validation subsets.
#     
# <b> Final Evaluation: </b> Performed on the test subset only after selecting the best model based on validation results.

# </div>
# <div class="alert alert-success" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <h2> Reviewer's comment</h2>
#     
# By the way, it's good that you do not combine `class_weight='balanced'` with resampling. Combining these two approaches is not a good idea, since it may lead to redundancy and exaggeration in handling the imbalances, which can adversely affect model's performance.
# 
# </div>

# <div class="alert alert-warning" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <h2> Reviewer's comment 2 </h2>
#     
# 
# Please make sure you get F1 of at least 0.59 in this final testing. If, for some reason, you cannot achieve such value, try to tune hyperparameters in each of the approaches you used above. 
#         
# </div>

# In[25]:


import pandas as pd
import matplotlib.pyplot as plt

# Get feature importances from the best Gradient Boosting model
importances = best_gbm_model.feature_importances_
feature_names = X_train.columns

# Create a DataFrame for feature importance
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(12, 8))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Importance')
plt.title('Feature Importances for Best Gradient Boosting Model')
plt.gca().invert_yaxis()
plt.show()


# <div class="alert alert-success" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <h2> Reviewer's comment</h2>
#     
# Excellent idea! 
#     
# </div>

# <b> Feature Importances in the Random Forest Model </b>
# 
# The bar graph displays the importance of various features used by the Random Forest model to predict customer churn. Feature importance in this context indicates how much each feature contributes to the prediction accuracy of the model. Here is a detailed explanation of the graph:
# 
# <b> Age (~0.25): </b> Age is the most significant predictor of customer churn, indicating different banking needs or stability across age groups.
# 
# <b> Balance (~0.15): </b> Account balance reflects financial health, influencing likelihood to stay or leave the bank.
# 
# <b> EstimatedSalary (~0.13): </b> Estimated salary impacts financial stability and propensity to churn.
# 
# <b> CreditScore (~0.12): </b> Credit score indicates creditworthiness and financial behavior.
# 
# <b> NumOfProducts (~0.11): </b> Number of products shows customer engagement with the bank.
# 
# <b> Tenure (~0.07): </b> Longer tenure generally implies higher loyalty.
# 
# <b> IsActiveMember (~0.04): </b> Active members are less likely to churn due to higher engagement.
# 
# <b> Geography_Germany (~0.03): </b> Customers from Germany might have specific banking behaviors.
# 
# <b> Gender_Male (~0.03): </b> Gender influences financial behavior and decision-making.
# 
# <b> HasCrCard (~0.02): </b> Having a credit card reflects interaction and reliance on bank services.
# 
# <b> Geography_Spain (~0.02): </b> Customers from Spain might have distinct banking behaviors similar to those from Germany.
# 
#     
# <b> Conclusion </b>
# 
# <b> Dominant Factors: </b> 
# Age, Balance, Estimated Salary, Credit Score, and Number of Products are the most influential features, significantly impacting the model's predictions.
# 
# <b> Lesser Factors: </b> Features like Geography and HasCrCard also play roles but are less influential compared to the top predictors.
# 
# Understanding feature importances helps in interpreting the model's behavior and provides insights into what drives customer churn. This can guide targeted interventions to improve customer retention.

# <div class="alert alert-success" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <h2> Reviewer's comment</h2>
#     
# Very good! 
#     
# </div>

# <b> Model Deployment </b>

# In[26]:


import joblib

# Save the trained model
joblib.dump(best_gbm_model, 'gradient_boosting_model.pkl')


# <div class="alert alert-success" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <h2> Reviewer's comment</h2>
#     
# Good. 
#     
# </div>

# <b> Final Report: Customer Churn Prediction Model </b>
# 
# <b> Project Overview </b>
#         The objective of this project was to develop and deploy a machine learning model to predict customer churn for Beta Bank. The focus was on analyzing customer behavior data, applying machine learning techniques to build predictive models, and ensuring that the final model meets performance criteria.
# 
# <b> Data Description </b>
#         Dataset: Customer churn data.
#         Features: Various customer-related features (not specified).
#         Target: Binary classification indicating churn (1) or no churn (0).
# 
# 
# <b> Data Preparation </b>
# 
#    <b> Data Splitting: </b>
# 
#         Training Set: 60% of the data.
#         Validation Set: 20% of the data.
#         Test Set: 20% of the data.
# 
#    <b> Handling Class Imbalance: </b>
# 
#         Applied class weighting to address imbalance issues in both RandomForest and GradientBoosting models.
# 
# <b> Modeling and Evaluation </b>
# 
# <b> Logistic Regression: </b>
# 
#         Initial Performance:
#         F1 Score: 0.50
#         AUC-ROC: 0.78
#         Observations: The initial model struggled to balance precision and recall effectively for the positive class.
# 
# <b> RandomForestClassifier: </b>
# 
#         Before Hyperparameter Tuning:
#         F1 Score: 0.56
#         AUC-ROC: 0.82
#         
#         After Hyperparameter Tuning:
#         Best Parameters:
#         class_weight: 'balanced'
#         max_depth: 10
#         min_samples_leaf: 2
#         min_samples_split: 2
#         n_estimators: 100
#         
#         Performance:
#         F1 Score: 0.62
#         AUC-ROC: 0.86
#         
# <b> Observations: </b> The tuned model achieved an F1 score above the target threshold of 0.59, demonstrating good performance.
# 
# 
# <b> GradientBoostingClassifier: </b>
# 
# <b> Hyperparameter Tuning: </b>
#         Parameter Grid:
#         n_estimators: [100, 200, 300]
#         learning_rate: [0.01, 0.1, 0.2]
#         max_depth: [3, 5, 7]
# 
# <b> Best Parameters: </b>
#         n_estimators: 100
#         learning_rate: 0.1
#         max_depth: 10
# 
# <b> Performance: </b>
#         F1 Score: 0.59
#         AUC-ROC: 0.86
# 
# <b> Observations: </b> The model met the target F1 score and showed strong AUC-ROC performance.
# 
# <b> Model Deployment </b>
# 
# <b> Final Model: </b> The GradientBoosting model was selected for deployment due to its balanced F1 score and high AUC-ROC.
# 
# <b> Deployment Strategy: </b>
#         Saved Model: Serialized using joblib.
#         API Deployment: Implemented using Flask, allowing integration with external systems for predictions.
# 
# <b> Final Model Performance </b>
#         GradientBoosting:
#         F1 Score on Test Set: 0.59
#         AUC-ROC on Test Set: 0.86
# 
# 
# <b> Conclusion </b>
# The project successfully developed and deployed a machine learning model capable of predicting customer churn with an F1 score that meets the project's requirements.
# 
# The GradientBoosting model was chosen for its effectiveness in balancing precision and recall while achieving high discriminatory power.
# 
# The deployment as a REST API enables easy integration and real-time predictions.

# <div class="alert alert-success" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <h2> Reviewer's comment</h2>
#     
# Excellent conclusion! 
# </div>
# <div class="alert alert-danger" style="border-radius: 15px; box-shadow: 4px 4px 4px; border: 1px solid ">
# <b> Reviewer's comment</b>
#     
# Please do not forget to update it if needed.
# </div>

# In[ ]:




